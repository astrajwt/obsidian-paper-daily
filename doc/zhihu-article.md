# 我写了一个 Obsidian 插件，解决 arXiv 每天 100+ 论文看不过来的问题

最近三个月一直在用自己写的这个工具，省了不少时间，想分享给同样有这个痛点的人。

---

## 问题：arXiv 的信息过载

做 AI/ML 的人都懂这种感受。

每天 arXiv 的 cs.AI + cs.LG + cs.CL 三个分类加在一起，轻松超过 100 篇新论文。你不可能每篇都看，但你也不敢完全不看——生怕错过一篇对自己方向很关键的工作。

于是很多人的工作流是这样的：每天花 30-60 分钟，在 arXiv 或者 Papers with Code 上刷一遍标题，感觉相关的点进去看 abstract，再判断要不要精读。这个过程枯燥、低效，而且严重依赖你那天的精神状态——注意力涣散的时候很容易漏掉重要的东西。

我自己的方向主要在 RL post-training、inference serving 和 agent 这几块，但每天刷 arXiv 的时候经常被不相关的论文分散注意力，真正关心的方向反而没看仔细。

所以我写了 **Paper Daily**——一个 Obsidian 插件，把这套流程自动化掉。

---

## 它是怎么工作的

整体思路很简单：每天早上 8:30，插件自动拉取 arXiv 新论文，用 LLM 批量打分，生成一份结构化的摘要，存进你的 Obsidian vault。你早饭后打开 Obsidian，直接读摘要就行了。

### 第一步：配置（一次性）

最关键的配置是**兴趣关键词**，这是整个系统的核心。你需要告诉插件你真正关心什么，以及关心的程度。

```json
{
  "interestKeywords": [
    { "keyword": "rlhf",             "weight": 5 },
    { "keyword": "grpo",             "weight": 5 },
    { "keyword": "reward model",     "weight": 4 },
    { "keyword": "agent",            "weight": 4 },
    { "keyword": "kv cache",         "weight": 3 },
    { "keyword": "speculative",      "weight": 3 },
    { "keyword": "moe",              "weight": 2 },
    { "keyword": "vllm",             "weight": 2 }
  ],
  "categories": ["cs.AI", "cs.LG", "cs.CL"],
  "maxResultsPerDay": 30
}
```

权重不要全部设成 5，要有梯度——这样排序才有意义。我的经验是把最核心的方向设 5，次要的设 2-3，感兴趣但不是重点的设 1。

LLM 方面，我用 DeepSeek，日常跑下来每天大约消耗 10-30k tokens，折合人民币 ¥0.01-0.03，基本可以忽略不计。如果你对摘要质量要求更高，换 Claude 也行，成本会高一些但分析深度明显好一个档次。

另外可以开启 **Deep Read** 功能：对评分最高的前 N 篇论文，插件会拉取完整的 HTML 正文，让 LLM 做更深入的逐篇分析，生成独立的阅读笔记并自动 wikilink 到日报里。这个功能我只对前 5 篇开，不然 token 消耗会上去。

### 第二步：每天自动运行

早上 8:30，插件在后台静默运行：

1. 拉取 arXiv 过去 30 小时内的新论文（覆盖时差问题）
2. 去重（已经出现过的论文不重复处理）
3. 对每篇论文计算兴趣分数：关键词命中 × 权重求和
4. LLM 批量分析，生成整体综述和每篇论文的简评
5. 对开启 Deep Read 的 Top 论文，抓取全文做深读
6. 全部写入 vault：`PaperDaily/inbox/2026-03-01.md`

整个过程你不需要做任何事。有一个浮动的进度悬浮窗，显示当前进度和 token 消耗，随时可以点 Stop 中止（中止是真的中止，不是假的——会 abort 正在进行的 LLM 请求）。

### 第三步：读摘要

生成的日报大概长这样：

```markdown
# Paper Daily — 2026-03-01

## 今日兴趣领域热度
1. RLHF & Post-training  ████████  (8 篇, 综合得分 142)
2. Inference Serving      █████     (5 篇, 综合得分 89)
3. Agentic RL             ████      (4 篇, 综合得分 71)

## AI 综述
今日 cs.AI/LG/CL 共收录 28 篇论文，RLHF 方向持续活跃……
（此处为 LLM 生成的叙述性总结）

## 精选论文（Deep Read）
1. [[2026-03-01 - Efficient GRPO for Long-Context RL]]
   - 兴趣命中：grpo × 2, reward model, rlhf
   - 一句话：提出了针对长上下文的 GRPO 变体，降低 50% 显存占用
   - 工程启示：……

## All Papers（全量，按分数排序）
| 标题 | 分数 | 命中关键词 | 方向 | 链接 |
|------|------|-----------|------|------|
| … | 28 | grpo, rlhf | Post-training | [arXiv]() |
```

**今日兴趣领域热度**这个模块是我最常用的。一眼就能看出今天你关心的方向有没有新动态，不需要自己在全量列表里找。如果你关心的方向热度很低，那就 5 分钟扫一眼 All Papers 表格就够了；如果某个方向突然有很多论文，再重点看精选。

所有内容都在 Obsidian vault 里，完全可搜索。Deep Read 生成的每篇论文分析笔记也是独立的 md 文件，可以 backlink、可以打标签、可以跟你自己的笔记互联——这是放在网页或者微信公众号上永远做不到的体验。

### 第四步：补录过去的日期

如果某天忘了跑，或者想补充最近一周的数据，插件提供 Backfill 功能。在命令面板里触发，选日期范围，插件会按天并行拉取并生成摘要。Backfill 在后台运行，不会影响今天的日报。

---

## 一些配置经验

**关键词设置是最值得花时间的地方。** 我第一版配置几乎全部设成权重 5，结果排序没有区分度，和随机差不多。后来花了一个下午重新梳理自己真正关心什么、关心程度如何，效果立刻好很多。

**不要贪多。** `maxResultsPerDay` 我设成 30，LLM 处理 30 篇的速度和质量都很好。设成 100 的话处理时间会变长，而且 LLM 对长列表的分析质量会下降。

**Deep Read 选 5-10 篇就够。** 每篇深读大概消耗 2-5k tokens，10 篇就是 20-50k，成本还是要考虑的。而且超过 10 篇的精读内容你可能也消化不完。

**LLM 选型建议：**
- 日常跑：DeepSeek-V3，便宜、速度快、中文输出好
- 想要更深的分析：Claude Sonnet，贵一些但 reasoning 明显更强
- 本地模型也支持（OpenAI 兼容接口），但质量参差不齐

---

## 离线也能用

网络只在拉取 arXiv 数据和调用 LLM 时用到。生成好的日报存在本地 vault，离线完全可以阅读。飞机上、地铁里翻历史日报都没问题。

---

## 总结

我自己用了三个月了，感受是：

- 每天花在"看今天有什么新论文"上的时间，从 30-60 分钟降到了 5-15 分钟
- 不再因为注意力被分散而漏掉关心方向的论文
- 历史日报积累起来之后，回溯某个方向的发展脉络变得很容易

如果你也在做 AI/ML 相关的工作，每天被 arXiv 轰炸，这个工具可能适合你。插件开源在 GitHub，Obsidian 社区插件市场也可以直接搜 **Paper Daily** 安装。

配置花半小时，之后基本不用管它。

---

*欢迎评论区交流兴趣关键词怎么配——这块确实有点 art，每个人的方向不一样，配置思路也会不同。*
